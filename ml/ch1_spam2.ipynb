{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import email\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')  # ikuya\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions to browse a list of words\n",
    "# ikuya\n",
    "def browse(L, name='(list)', num_samples=5):\n",
    "    n = num_samples if len(L)>=num_samples else len(L)\n",
    "    print('%s: len=%d, samples: %s' % (name, len(L), ' '.join(L[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations: len=32, samples: ! \" # $ %\n",
      "stopwords: len=179, samples: until yourself all mustn't him\n"
     ]
    }
   ],
   "source": [
    "browse(punctuations, 'punctuations')\n",
    "browse(list(stopwords), 'stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to load and parse emails\n",
    "\n",
    "# Combine the different parts of the email into a flat list of strings\n",
    "def flatten_to_string(parts):\n",
    "    ret = []\n",
    "    if type(parts) == str:\n",
    "        ret.append(parts)\n",
    "    elif type(parts) == list:\n",
    "        for part in parts:\n",
    "            ret += flatten_to_string(part)\n",
    "    elif parts.get_content_type == 'text/plain':\n",
    "        ret += parts.get_payload()\n",
    "    return ret\n",
    "\n",
    "# Extract subject and body text from a single email file\n",
    "def extract_email_text(path):\n",
    "    # Load a single email from an input file\n",
    "    with open(path, errors='ignore') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "    \n",
    "    # Read the email subject\n",
    "    subject = msg['Subject']\n",
    "    if not subject:\n",
    "        subject = \"\"\n",
    "    \n",
    "    # Read the email body\n",
    "    body = ' '.join(m for m in flatten_to_string(msg.get_payload())\n",
    "                    if type(m) == str)\n",
    "    if not body:\n",
    "        body = \"\"\n",
    "    \n",
    "    return subject + ' ' + body\n",
    "\n",
    "# Process a single email file into stemmed tokens\n",
    "def load(path):\n",
    "    email_text = extract_email_text(path)\n",
    "    if not email_text:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the message\n",
    "    tokens = nltk.word_tokenize(email_text)\n",
    "    \n",
    "    # Remove punctuation from tokens\n",
    "    tokens = [i.strip(\"\".join(punctuations)) for i in tokens\n",
    "             if i not in punctuations]\n",
    "    \n",
    "    # Remove stopwords and stem tokens\n",
    "    if len(tokens) > 2:\n",
    "        return [stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'datasets/trec07p/data/'\n",
    "LABELS_FILE = 'datasets/trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "\n",
    "nltk.download('punkt')  # ikuya\n",
    "\n",
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()\n",
    "\n",
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower()=='ham' else 0\n",
    "\n",
    "# Split corpus into training and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "num_train = int(len(filelist) * TRAINING_SET_RATIO)\n",
    "X_train = filelist[:num_train]\n",
    "X_test  = filelist[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jaccard 類似度\n",
    "    - $J(S, T) = |S \\cap T| / |S \\cup T|$\n",
    "    - まったく違う（共通要素がない）なら 0、まったく同じなら 1、と正規化される\n",
    "    - S, T の要素数に大きな差がある（＝一方が極端に大きい）と直感に反した値になる\n",
    "- MinHash\n",
    "    - 二つの集合の Jaccard 類似度の近似値を得る手法。集合の sketch のみあれば計算できるので、集合そのものを保存するより保存領域が少なくて済む。\n",
    "    - Andrei Z. Broder, http://cs.brown.edu/courses/cs253/papers/nearduplicate.pdf, 2000\n",
    "    - 集合$S$の部分集合$S_A$と$S_B$があったとき、$S_A$と$S_B$のJaccard類似度は、ランダム置換 $\\pi$ が $min(\\pi(S_A)) == min(\\pi(S_B))$ である確率に等しい。（$\\pi$は$S→S$のランダムな置換）\n",
    "    - そこで、たとえば $n=100$ 個の置換 ${ \\pi_1, \\pi_2, \\ldots, \\pi_n }$に対する ${ min(\\pi_1(S_A)), min(\\pi_2(S_A)), \\ldots, min(\\pi_n(S_A)) }$を集合 $S_A$ のsketchとし、同様に計算した $S_B$ のsketchといくつの要素が一致するか調べると、それがJaccard類似度の近似となる。\n",
    "    - 実用的には S の要素は n-shingle (単語版の n-gram で、連続した n 語の列）などを使う（文書の場合）。\n",
    "    - 実用的にはランダム置換 $\\pi$ に（ソルト入りの）ハッシュ関数を使う。ハッシュ値の最小値を保存するため MinHash と呼ばれる（たぶん）。\n",
    "- Locality Sensitive Hashing (LSH)\n",
    "    - 概要\n",
    "        - 対象のアイテム群の中から、クエリのアイテムと距離が近い（＝類似した）アイテム群（の部分集合）を効率よく（ただし確率的に）見つける方法。\n",
    "        - 文書検索などにおいて、大量（n個）の文書群から類似した文書を見つけるには、素朴には少なくとも O(n) の比較が必要だが、これを速くしたい。二分検索等が使えるハッシュ値であれば高速に見つけることができる、という性質を使う。\n",
    "    - 手法\n",
    "        - 基本的なアイデアは、アイテムとアイテムが近ければ一致する確率が高い locality sensitive なハッシュ関数 h(.) を複数個用意し、各アイテムに対して複数のハッシュ値を保存しておいて、クエリアイテムのハッシュ値群と多く一致すれば類似していると見なす。\n",
    "        - locality sensitive なハッシュ関数は、アイテムの性質やアイテム間での距離の定義に応じて設計する必要がある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Extract only spam files for inserting into the LSH matcher\n",
    "spam_files = [x for x in X_train if labels[x] == 0]\n",
    "\n",
    "# Initialize MinHashLSH matcher with a Jaccard\n",
    "# threshold of 0.5 and 128 MinHash Permutation functions\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "# Populate the LSH matcher with training spam MinHashes\n",
    "for idx, f in enumerate(spam_files):\n",
    "    #print(idx, f)\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    stems = load(os.path.join(DATA_DIR, f))\n",
    "    if len(stems) < 2: continue\n",
    "    for s in stems:\n",
    "        minhash.update(s.encode('utf-8'))\n",
    "    lsh.insert(f, minhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_predict_label(lsh, stems):\n",
    "    '''\n",
    "    Queries the LSH matcher and returns:\n",
    "        0 if predicted spam\n",
    "        1 if predicted ham\n",
    "       -1 if parsing error\n",
    "    '''\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    if len(stems) < 2:\n",
    "        return -1\n",
    "    for s in stems:\n",
    "        minhash.update(s.encode('utf-8'))\n",
    "    matches = lsh.query(minhash)\n",
    "    if matches:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, lsh):\n",
    "    n = len(X)\n",
    "    y_truth = [-1] * n\n",
    "    y_predict = [-1] * n\n",
    "    for i in range(n):\n",
    "        filename = X[i]\n",
    "        label = labels[filename]\n",
    "        y_truth[i] = label\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        stems = load(path)\n",
    "        y_predict[i] = lsh_predict_label(lsh, stems)\n",
    "    return (y_truth, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15760/22626\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(y_truth, y_predict) = predict(X_test, lsh)\n",
    "print(\"%d/%d\" % (sum([a==b for a,b in zip(y_truth, y_predict)]), len(y_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :          6439,          136\n",
      "Actual SPAM:          5283,        10768\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         28.5%,        0.6%\n",
      "Actual SPAM:         23.3%,        47.6%\n",
      "\n",
      "accuracy = 76.0%\n"
     ]
    }
   ],
   "source": [
    "def percent(n, total):\n",
    "    return n / total * 100.0\n",
    "    \n",
    "def print_result(y_truth, y_predict):\n",
    "    ham_ham = ham_spam = spam_ham = spam_spam = 0\n",
    "    confusion_matrix = (ham_ham, ham_spam, spam_ham, spam_spam)\n",
    "    \n",
    "    for (truth, predict) in zip(y_truth, y_predict):\n",
    "        if truth==1:\n",
    "            if predict==1:\n",
    "                ham_ham += 1\n",
    "            else:\n",
    "                ham_spam += 1\n",
    "        else:\n",
    "            if predict==1:\n",
    "                spam_ham += 1\n",
    "            else:\n",
    "                spam_spam += 1\n",
    "    \n",
    "    print(\"#y_truth, #y_predict = %d, %d\" % (len(y_truth), len(y_predict)))\n",
    "    num_predicted = ham_ham + ham_spam + spam_ham + spam_spam\n",
    "    \n",
    "    def pct(n):\n",
    "        return percent(n, num_predicted)\n",
    "    \n",
    "    print(\"             Predicted HAM, Predicted SPAM\")\n",
    "    print(\"Actual HAM :         %5d,        %5d\" % (ham_ham, ham_spam))\n",
    "    print(\"Actual SPAM:         %5d,        %5d\" % (spam_ham, spam_spam))\n",
    "    print()\n",
    "    print(\"             Predicted HAM, Predicted SPAM\")\n",
    "    print(\"Actual HAM :         %2.1f%%,        %2.1f%%\" % (pct(ham_ham), pct(ham_spam)))\n",
    "    print(\"Actual SPAM:         %2.1f%%,        %2.1f%%\" % (pct(spam_ham), pct(spam_spam)))\n",
    "    print()\n",
    "    print(\"accuracy = %2.1f%%\" % (pct(ham_ham + spam_spam)))\n",
    "\n",
    "print_result(y_truth, y_predict)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Word blacklisting\n",
    "\n",
    "#y_truth, #y_predict = 22626, 22626\n",
    "             Predicted HAM, Predicted SPAM\n",
    "Actual HAM :          5387,         1188\n",
    "Actual SPAM:          5828,        10223\n",
    "\n",
    "             Predicted HAM, Predicted SPAM\n",
    "Actual HAM :         23.8%,        5.3%\n",
    "Actual SPAM:         25.8%,        45.2%\n",
    "\n",
    "accuracy = 69.0%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### MinHash + LSH\n",
    "\n",
    "#y_truth, #y_predict = 22626, 22626\n",
    "             Predicted HAM, Predicted SPAM\n",
    "Actual HAM :          6439,          136\n",
    "Actual SPAM:          5283,        10768\n",
    "\n",
    "             Predicted HAM, Predicted SPAM\n",
    "Actual HAM :         28.5%,        0.6%\n",
    "Actual SPAM:         23.3%,        47.6%\n",
    "\n",
    "accuracy = 76.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all (ham, spam) = (25220, 50199)\n",
      "all (ham, spam) = (33.4%, 66.6%)\n"
     ]
    }
   ],
   "source": [
    "# ham/spam ratio in all emails\n",
    "num_hams  = sum([1 for label in labels.values() if label==1])\n",
    "num_spams = sum([1 for label in labels.values() if label==0])\n",
    "num_total = num_hams + num_spams\n",
    "print(\"all (ham, spam) = (%d, %d)\" % (num_hams, num_spams))\n",
    "print(\"all (ham, spam) = (%2.1f%%, %2.1f%%)\" % (percent(num_hams, num_total), percent(num_spams, num_total)))\n",
    "\n",
    "# → 全データの ham/spam 割合はおよそ 33/67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (ham, spam) = (6575, 16051)\n",
      "test (ham, spam) = (29.1%, 70.9%)\n"
     ]
    }
   ],
   "source": [
    "# ham/spam ratio in test emails\n",
    "y_test = [labels[fn] for fn in X_test]\n",
    "num_hams  = sum([1 for label in y_test if label==1])\n",
    "num_spams = sum([1 for label in y_test if label==0])\n",
    "num_total = num_hams + num_spams\n",
    "print(\"test (ham, spam) = (%d, %d)\" % (num_hams, num_spams))\n",
    "print(\"test (ham, spam) = (%2.1f%%, %2.1f%%)\" % (percent(num_hams, num_total), percent(num_spams, num_total)))\n",
    "\n",
    "# → テストデータの ham/spam 割合はおよそ 30/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :             0,         6575\n",
      "Actual SPAM:             0,        16051\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         0.0%,        29.1%\n",
      "Actual SPAM:         0.0%,        70.9%\n",
      "\n",
      "accuracy = 70.9%\n"
     ]
    }
   ],
   "source": [
    "y_truth_2 = y_truth\n",
    "y_predict_2 = [0] * len(y_truth_2)\n",
    "print_result(y_truth_2, y_predict_2)\n",
    "# → 全部 spam (0) と判定しても accuracy は 70% になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :          3265,         3310\n",
      "Actual SPAM:          7965,         8086\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         14.4%,        14.6%\n",
      "Actual SPAM:         35.2%,        35.7%\n",
      "\n",
      "accuracy = 50.2%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "y_truth_3 = y_truth\n",
    "y_predict_3 = [random.randint(0, 1) for _ in y_truth_3]\n",
    "print_result(y_truth_3, y_predict_3)\n",
    "# → 0/1 五分五分で判定すると accuracy は 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
