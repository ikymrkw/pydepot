{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import email\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')  # ikuya\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions to browse a list of words\n",
    "# ikuya\n",
    "def browse(L, name='(list)', num_samples=5):\n",
    "    n = num_samples if len(L)>=num_samples else len(L)\n",
    "    print('%s: len=%d, samples: %s' % (name, len(L), ' '.join(L[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations: len=32, samples: ! \" # $ %\n",
      "stopwords: len=179, samples: my until of in hasn\n"
     ]
    }
   ],
   "source": [
    "browse(punctuations, 'punctuations')\n",
    "browse(list(stopwords), 'stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to load and parse emails\n",
    "\n",
    "# Combine the different parts of the email into a flat list of strings\n",
    "def flatten_to_string(parts):\n",
    "    ret = []\n",
    "    if type(parts) == str:\n",
    "        ret.append(parts)\n",
    "    elif type(parts) == list:\n",
    "        for part in parts:\n",
    "            ret += flatten_to_string(part)\n",
    "    elif parts.get_content_type == 'text/plain':\n",
    "        ret += parts.get_payload()\n",
    "    return ret\n",
    "\n",
    "# Extract subject and body text from a single email file\n",
    "def extract_email_text(path):\n",
    "    # Load a single email from an input file\n",
    "    with open(path, errors='ignore') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "    \n",
    "    # Read the email subject\n",
    "    subject = msg['Subject']\n",
    "    if not subject:\n",
    "        subject = \"\"\n",
    "    \n",
    "    # Read the email body\n",
    "    body = ' '.join(m for m in flatten_to_string(msg.get_payload())\n",
    "                    if type(m) == str)\n",
    "    if not body:\n",
    "        body = \"\"\n",
    "    \n",
    "    return subject + ' ' + body\n",
    "\n",
    "# Process a single email file into stemmed tokens\n",
    "PUNCTS = \"\".join(punctuations)\n",
    "def load(path):\n",
    "    email_text = extract_email_text(path)\n",
    "    if not email_text:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the message\n",
    "    tokens = nltk.word_tokenize(email_text)\n",
    "    \n",
    "    # Remove punctuation from tokens\n",
    "    tokens = [i.strip(PUNCTS) for i in tokens if i not in punctuations]\n",
    "    \n",
    "    # Remove stopwords and stem tokens\n",
    "    if len(tokens) > 2:\n",
    "        return [stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'datasets/trec07p/data/'\n",
    "LABELS_FILE = 'datasets/trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "\n",
    "nltk.download('punkt')  # ikuya\n",
    "\n",
    "labels = {}\n",
    "\n",
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower()=='ham' else 0\n",
    "\n",
    "# Split corpus into training and test sets\n",
    "#filelist = os.listdir(DATA_DIR)\n",
    "#num_train = int(len(filelist) * TRAINING_SET_RATIO)\n",
    "#X_train = filelist[:num_train]\n",
    "#X_test  = filelist[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def read_email_files():\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(labels)):\n",
    "        filename = 'inmail.' + str(i+1)\n",
    "        email_str = extract_email_text(os.path.join(DATA_DIR, filename))\n",
    "        X.append(email_str)\n",
    "        y.append(labels[filename])\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = read_email_files()\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = \\\n",
    "    train_test_split(X, y, range(len(y)),\n",
    "                     train_size=TRAINING_SET_RATIO, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#(X_train, X_test, y_train, y_test, idx_train, idx_test) = (52793, 22626, 52793, 22626, 52793, 22626)\n"
     ]
    }
   ],
   "source": [
    "print(\"#(X_train, X_test, y_train, y_test, idx_train, idx_test) =\",\n",
    "      (len(X_train), len(X_test), len(y_train), len(y_test), len(idx_train), len(idx_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vector = vectorizer.fit_transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vector = vectorizer.fit_transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "単語 $t_i$ の文書 $d_j$ における単語の重要度 $tfidf_{i,j}= tf_{i,j} \\cdot idf_i$\n",
    "\n",
    "$tf_{i,j}$ は単語 $t_i$ の出現頻度 (term frequency) で $tf_{i, j} = n_{i,j} / \\Sigma_k{ n_{k,j} }$、ただし $n_{i,j}$ は文書 $d_j$ における単語 $t_i$ の出現回数。\n",
    "\n",
    "$idf_i$ は単語 $t_i$ の逆文書頻度 (inverse document frequency) で、全文書に登場する語では 0 に、1/e (≒ 37%) の文書に登場する語では 1 に、という値をとる単調増加関数。多くの文書に登場する語のスコアを下げる役割をもつ。 $idf_i = log{ \\frac{|D|}{|\\{d:d∋t_i\\}|} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.973\n",
      "Wall time: 59.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the classifier and make label predictions\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vector, y_train)\n",
    "y_pred = mnb.predict(X_test_vector)\n",
    "\n",
    "# Print results\n",
    "print('Accuracy {:.3f}'.format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer => Accuracy 0.955, Wall time: 77.8 ms\n",
    "- TfidfVectorizer => Accuracy 0.973, Wall time: 59.8 ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
