{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import email\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')  # ikuya\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions to browse a list of words\n",
    "# ikuya\n",
    "def browse(L, name='(list)', num_samples=5):\n",
    "    n = num_samples if len(L)>=num_samples else len(L)\n",
    "    print('%s: len=%d, samples: %s' % (name, len(L), ' '.join(L[:n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations: len=32, samples: ! \" # $ %\n",
      "stopwords: len=179, samples: after an each doing these\n"
     ]
    }
   ],
   "source": [
    "browse(punctuations, 'punctuations')\n",
    "browse(list(stopwords), 'stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to load and parse emails\n",
    "\n",
    "# Combine the different parts of the email into a flat list of strings\n",
    "def flatten_to_string(parts):\n",
    "    ret = []\n",
    "    if type(parts) == str:\n",
    "        ret.append(parts)\n",
    "    elif type(parts) == list:\n",
    "        for part in parts:\n",
    "            ret += flatten_to_string(part)\n",
    "    elif parts.get_content_type == 'text/plain':\n",
    "        ret += parts.get_payload()\n",
    "    return ret\n",
    "\n",
    "# Extract subject and body text from a single email file\n",
    "def extract_email_text(path):\n",
    "    # Load a single email from an input file\n",
    "    with open(path, errors='ignore') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "    \n",
    "    # Read the email subject\n",
    "    subject = msg['Subject']\n",
    "    if not subject:\n",
    "        subject = \"\"\n",
    "    \n",
    "    # Read the email body\n",
    "    body = ' '.join(m for m in flatten_to_string(msg.get_payload())\n",
    "                    if type(m) == str)\n",
    "    if not body:\n",
    "        body = \"\"\n",
    "    \n",
    "    return subject + ' ' + body\n",
    "\n",
    "# Process a single email file into stemmed tokens\n",
    "PUNCTS = \"\".join(punctuations)\n",
    "def load(path):\n",
    "    email_text = extract_email_text(path)\n",
    "    if not email_text:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the message\n",
    "    tokens = nltk.word_tokenize(email_text)\n",
    "    \n",
    "    # Remove punctuation from tokens\n",
    "    tokens = [i.strip(PUNCTS) for i in tokens if i not in punctuations]\n",
    "    \n",
    "    # Remove stopwords and stem tokens\n",
    "    if len(tokens) > 2:\n",
    "        return [stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load dataset\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'datasets/trec07p/data/'\n",
    "LABELS_FILE = 'datasets/trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "\n",
    "nltk.download('punkt')  # ikuya\n",
    "\n",
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()\n",
    "\n",
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower()=='ham' else 0\n",
    "\n",
    "# Split corpus into training and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "num_train = int(len(filelist) * TRAINING_SET_RATIO)\n",
    "X_train = filelist[:num_train]\n",
    "X_test  = filelist[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Learn (extract ham_words and spam_words)\n",
    "for filename in X_train:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        label = labels[filename]\n",
    "        stems = load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "        if label == 1:\n",
    "            ham_words.update(stems)\n",
    "        elif label == 0:\n",
    "            spam_words.update(stems)\n",
    "        else:\n",
    "            continue\n",
    "blacklist = spam_words - ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#X_train=52793, #X_test=22626\n",
      "ham_words: len=187154, samples:  gpo_copy_fil www.wbir.com/news/national/story.asp slashdot.org/comments.pl parlato\n",
      "spam_words: len=122577, samples:  事務局以外でサポートとして、私個人の心情が入りメールしています。 myrangeinternet.com/x/mjuxodm5ndk5|mzezmjg4|chjvzhvjdhrlc3rwyw5lbebzcgvlzhkudxdhdgvybg9vlmnh|mjm0odi=|ng==|||.html ｻﾘｱｨｴﾊｩﾋﾄ｣ｺﾍｨｹﾐｽｳｷﾖｲ羚ﾖﾀ犲ﾍｽ盪ｹｵﾄｲ霻ｻｯﾔｭﾔｹｽｨﾖﾐｸﾟｲ羯ﾜﾀ柀ﾋﾔｱﾐｽｳﾌ袞ｵ www.4ulanesdealer.com/i/svrlha/y13s3gzy/viloy_5971/kaleh_15.jpg\n",
      "blacklist: len=99287, samples: myrangeinternet.com/x/mjuxodm5ndk5|mzezmjg4|chjvzhvjdhrlc3rwyw5lbebzcgvlzhkudxdhdgvybg9vlmnh|mjm0odi=|ng==|||.html 事務局以外でサポートとして、私個人の心情が入りメールしています。 ｻﾘｱｨｴﾊｩﾋﾄ｣ｺﾍｨｹﾐｽｳｷﾖｲ羚ﾖﾀ犲ﾍｽ盪ｹｵﾄｲ霻ｻｯﾔｭﾔｹｽｨﾖﾐｸﾟｲ羯ﾜﾀ柀ﾋﾔｱﾐｽｳﾌ袞ｵ www.4ulanesdealer.com/i/svrlha/y13s3gzy/viloy_5971/kaleh_15.jpg a/nod/to/the/fact/that/the/university/is/located/on/the/former/site/of/leland/stanford/s/horse/farm.//the/university/s/founding/grant/was/written/on/november/11\n"
     ]
    }
   ],
   "source": [
    "# ham, spam, blacklist の数を見てみる。 blacklist の中身を見てみる。\n",
    "print(\"#X_train=%d, #X_test=%d\" % (len(X_train), len(X_test)))\n",
    "browse(list(ham_words), \"ham_words\")\n",
    "browse(list(spam_words), \"spam_words\")\n",
    "browse(list(blacklist), \"blacklist\")\n",
    "\n",
    "#blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rovinci',\n",
       " 'rovinc',\n",
       " 'rovidi',\n",
       " 'rovid',\n",
       " 'rovesciano',\n",
       " 'roversi',\n",
       " 'rovement',\n",
       " 'roux',\n",
       " 'routledg',\n",
       " 'routine痴',\n",
       " 'roust',\n",
       " 'rousseau',\n",
       " 'rouss',\n",
       " 'roushil',\n",
       " 'rousettu',\n",
       " 'rous=3dmze3njeyn0a4ota4qdiynka',\n",
       " 'rous=3dmze3njeyn0a4nzy3qdiynka',\n",
       " 'rous=3dmze3njeyn0a4njqzqdiynka',\n",
       " 'rourk',\n",
       " 'roup']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blacklist から明らかに数値やURLっぽいものを除いて、中身を見てみる\n",
    "import re\n",
    "nums = re.compile('([\\\\d:,=]+|\\\\d[\\\\da-fA-F]+)')\n",
    "bb = blacklist.copy()\n",
    "bb = [w for w in bb if '/' not in w and '.' not in w and '-' not in w]\n",
    "bb = [w for w in bb if not nums.fullmatch(w)]\n",
    "bb.sort(reverse=True)\n",
    "offset, length = int(len(bb)/3), 20\n",
    "bb[offset:offset+length]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, blackset):\n",
    "    n = len(X)\n",
    "    y_truth = [-1] * n\n",
    "    y_predict = [-1] * n\n",
    "    for i in range(n):\n",
    "        filename = X[i]\n",
    "        label = labels[filename]\n",
    "        y_truth[i] = label\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        stems = load(path)\n",
    "        y_predict[i] = 1\n",
    "        for w in stems:\n",
    "            if w in blackset:\n",
    "                y_predict[i] = 0\n",
    "                break\n",
    "        #y_predict[i] = int(bool(set(stems) & blackset))\n",
    "    return (y_truth, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14276/22626\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "(y_truth, y_predict) = predict(X_test, blacklist)\n",
    "print(\"%d/%d\" % (sum([a==b for a,b in zip(y_truth, y_predict)]), len(y_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :          5436,         1139\n",
      "Actual SPAM:          7211,         8840\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         24.0%,        5.0%\n",
      "Actual SPAM:         31.9%,        39.1%\n",
      "\n",
      "accuracy = 63.1%\n"
     ]
    }
   ],
   "source": [
    "def percent(n, total):\n",
    "    return n / total * 100.0\n",
    "    \n",
    "def print_result(y_truth, y_predict):\n",
    "    ham_ham = ham_spam = spam_ham = spam_spam = 0\n",
    "    confusion_matrix = (ham_ham, ham_spam, spam_ham, spam_spam)\n",
    "    \n",
    "    for (truth, predict) in zip(y_truth, y_predict):\n",
    "        if truth==1:\n",
    "            if predict==1:\n",
    "                ham_ham += 1\n",
    "            else:\n",
    "                ham_spam += 1\n",
    "        else:\n",
    "            if predict==1:\n",
    "                spam_ham += 1\n",
    "            else:\n",
    "                spam_spam += 1\n",
    "    \n",
    "    print(\"#y_truth, #y_predict = %d, %d\" % (len(y_truth), len(y_predict)))\n",
    "    num_predicted = ham_ham + ham_spam + spam_ham + spam_spam\n",
    "    \n",
    "    def pct(n):\n",
    "        return percent(n, num_predicted)\n",
    "    \n",
    "    print(\"             Predicted HAM, Predicted SPAM\")\n",
    "    print(\"Actual HAM :         %5d,        %5d\" % (ham_ham, ham_spam))\n",
    "    print(\"Actual SPAM:         %5d,        %5d\" % (spam_ham, spam_spam))\n",
    "    print()\n",
    "    print(\"             Predicted HAM, Predicted SPAM\")\n",
    "    print(\"Actual HAM :         %2.1f%%,        %2.1f%%\" % (pct(ham_ham), pct(ham_spam)))\n",
    "    print(\"Actual SPAM:         %2.1f%%,        %2.1f%%\" % (pct(spam_ham), pct(spam_spam)))\n",
    "    print()\n",
    "    print(\"accuracy = %2.1f%%\" % (pct(ham_ham + spam_spam)))\n",
    "\n",
    "print_result(y_truth, y_predict)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " #y_truth, #y_predict = 22626, 22626\n",
    "              Predicted HAM, Predicted SPAM\n",
    " Actual HAM :          5387,         1188\n",
    " Actual SPAM:          5828,        10223\n",
    " \n",
    "              Predicted HAM, Predicted SPAM\n",
    " Actual HAM :         23.8%,        5.3%\n",
    " Actual SPAM:         25.8%,        45.2%\n",
    " \n",
    " accuracy = 69.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all (ham, spam) = (25220, 50199)\n",
      "all (ham, spam) = (33.4%, 66.6%)\n"
     ]
    }
   ],
   "source": [
    "# ham/spam ratio in all emails\n",
    "num_hams  = sum([1 for label in labels.values() if label==1])\n",
    "num_spams = sum([1 for label in labels.values() if label==0])\n",
    "num_total = num_hams + num_spams\n",
    "print(\"all (ham, spam) = (%d, %d)\" % (num_hams, num_spams))\n",
    "print(\"all (ham, spam) = (%2.1f%%, %2.1f%%)\" % (percent(num_hams, num_total), percent(num_spams, num_total)))\n",
    "\n",
    "# → 全データの ham/spam 割合はおよそ 33/67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (ham, spam) = (6575, 16051)\n",
      "test (ham, spam) = (29.1%, 70.9%)\n"
     ]
    }
   ],
   "source": [
    "# ham/spam ratio in test emails\n",
    "y_test = [labels[fn] for fn in X_test]\n",
    "num_hams  = sum([1 for label in y_test if label==1])\n",
    "num_spams = sum([1 for label in y_test if label==0])\n",
    "num_total = num_hams + num_spams\n",
    "print(\"test (ham, spam) = (%d, %d)\" % (num_hams, num_spams))\n",
    "print(\"test (ham, spam) = (%2.1f%%, %2.1f%%)\" % (percent(num_hams, num_total), percent(num_spams, num_total)))\n",
    "\n",
    "# → テストデータの ham/spam 割合はおよそ 30/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :             0,         6575\n",
      "Actual SPAM:             0,        16051\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         0.0%,        29.1%\n",
      "Actual SPAM:         0.0%,        70.9%\n",
      "\n",
      "accuracy = 70.9%\n"
     ]
    }
   ],
   "source": [
    "y_truth_2 = y_truth\n",
    "y_predict_2 = [0] * len(y_truth_2)\n",
    "print_result(y_truth_2, y_predict_2)\n",
    "# → 全部 spam (0) と判定しても accuracy は 70% になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#y_truth, #y_predict = 22626, 22626\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :          3265,         3310\n",
      "Actual SPAM:          7965,         8086\n",
      "\n",
      "             Predicted HAM, Predicted SPAM\n",
      "Actual HAM :         14.4%,        14.6%\n",
      "Actual SPAM:         35.2%,        35.7%\n",
      "\n",
      "accuracy = 50.2%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "y_truth_3 = y_truth\n",
    "y_predict_3 = [random.randint(0, 1) for _ in y_truth_3]\n",
    "print_result(y_truth_3, y_predict_3)\n",
    "# → 0/1 五分五分で判定すると accuracy は 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
